<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Road to NTK (1) : Introduction | John has Won</title>
<meta name="keywords" content="NTK, ML, Math">
<meta name="description" content="1. Double Descent and Overparameterization In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,
The (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data.">
<meta name="author" content="John Won">
<link rel="canonical" href="https://periphanes.github.org/gntk_test_1">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5c25c975546c048d1a5600aadb48425ae1bc921a9a18fe67d6955c9535260811.css" integrity="sha256-XCXJdVRsBI0aVgCq20hCWuG8khqaGP5n1pVclTUmCBE=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://periphanes.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://periphanes.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://periphanes.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://periphanes.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://periphanes.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Road to NTK (1) : Introduction" />
<meta property="og:description" content="1. Double Descent and Overparameterization In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,
The (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://periphanes.github.io/ntk/ntk_1/" /><meta property="article:section" content="ntk" />
<meta property="article:published_time" content="2023-10-20T11:30:03+00:00" />
<meta property="article:modified_time" content="2023-10-20T11:30:03+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Road to NTK (1) : Introduction"/>
<meta name="twitter:description" content="1. Double Descent and Overparameterization In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,
The (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ntks",
      "item": "https://periphanes.github.io/ntk/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Road to NTK (1) : Introduction",
      "item": "https://periphanes.github.io/ntk/ntk_1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Road to NTK (1) : Introduction",
  "name": "Road to NTK (1) : Introduction",
  "description": "1. Double Descent and Overparameterization In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,\nThe (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data.",
  "keywords": [
    "NTK", "ML", "Math"
  ],
  "articleBody": "1. Double Descent and Overparameterization In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,\nThe (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data. However, in recent years, it has become commonplace to train highly over-parameterized models, with parameter counts orders of magnitude higher than the actual training data point numbers. These models aim to achieve near-zero training error, yet still achieve high performances on testing data. This kind of contradiction is resolved by (Belkin et al., 2018) [1] by the new updated risk curve, aptly named the ‘double descent’ risk curve. In this formultation, the traditional U-shaped curve is still shown for lower parameter counts, but as the model complexity increases past the point where the model can perfectly fit the training data (passing the interpolation threshold), testing error starts to drop again.\nWhile there are some theories regarding what makes such phenomena happen, there has yet to be a satisfactory conclusion. Some argue that models in the interpolating regime are indeed overfitted, but\n2. Infinite Width-Networks and the Neural Tangent Kernel REFERENCES\n[1] Belkin, M., Hsu, D., Ma, S., \u0026 Mandal, S. Reconciling modern machine learning practice and the bias-variance tradeoff, Proceedings of the National Academy of Sciences, (2019)\n",
  "wordCount" : "277",
  "inLanguage": "en",
  "datePublished": "2023-10-20T11:30:03Z",
  "dateModified": "2023-10-20T11:30:03Z",
  "author":{
    "@type": "Person",
    "name": "John Won"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://periphanes.github.io/ntk/ntk_1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John has Won",
    "logo": {
      "@type": "ImageObject",
      "url": "https://periphanes.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://periphanes.github.io/" accesskey="h" title="John has Won (Alt + H)">John has Won</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://periphanes.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://periphanes.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://periphanes.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://periphanes.github.io/ntk/">Ntks</a></div>
    <h1 class="post-title">
      Road to NTK (1) : Introduction
    </h1>
    <div class="post-meta"><span title='2023-10-20 11:30:03 +0000 +0000'>October 20, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;277 words&nbsp;·&nbsp;John Won

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-double-descent-and-overparameterization">1. Double Descent and Overparameterization</a></li>
    <li><a href="#2-infinite-width-networks-and-the-neural-tangent-kernel">2. Infinite Width-Networks and the Neural Tangent Kernel</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="1-double-descent-and-overparameterization">1. Double Descent and Overparameterization<a hidden class="anchor" aria-hidden="true" href="#1-double-descent-and-overparameterization">#</a></h2>
<p>In classical theories of machine learning and statistical learning, there was the traditional consensus that there was a careful balance between the training error, and generalization gap. We are familiar with the U-shaped graph of the test risk when the complexity of a model is increased. The expected formulation is an initial decrease of both training and test risk,</p>
<p>The (a) diagram in the figure above showcases this old wisdom, with training risk being reduced as model parameter counts increase, while the test risk rebounds upwards again as the model overfits to the training data. However, in recent years, it has become commonplace to train highly over-parameterized models, with parameter counts orders of magnitude higher than the actual training data point numbers. These models aim to achieve near-zero training error, yet still achieve high performances on testing data. This kind of contradiction is resolved by (Belkin et al., 2018) [1] by the new updated risk curve, aptly named the ‘double descent’ risk curve. In this formultation, the traditional U-shaped curve is still shown for lower parameter counts, but as the model complexity increases past the point where the model can perfectly fit the training data (passing the <em>interpolation</em> threshold), testing error starts to drop again.</p>
<p>While there are some theories regarding what makes such phenomena happen, there has yet to be a satisfactory conclusion. Some argue that models in the interpolating regime are indeed overfitted, but</p>
<h2 id="2-infinite-width-networks-and-the-neural-tangent-kernel">2. Infinite Width-Networks and the Neural Tangent Kernel<a hidden class="anchor" aria-hidden="true" href="#2-infinite-width-networks-and-the-neural-tangent-kernel">#</a></h2>
<p><em>REFERENCES</em></p>
<p>[1] Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S. <em>Reconciling modern machine learning practice and the bias-variance tradeoff,</em> Proceedings of the National Academy of Sciences, (2019)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://periphanes.github.io/tags/ntk/">NTK</a></li>
      <li><a href="https://periphanes.github.io/tags/ml/">ML</a></li>
      <li><a href="https://periphanes.github.io/tags/math/">Math</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://periphanes.github.io/">John has Won</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
